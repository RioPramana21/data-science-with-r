---
title: "group1-project"
output: html_document
date: "2023-05-09"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE, result = FALSE)
```
We are sorry that the final report is super shortened because our code + all main graphs/outputs can exceed 50 pages. So for this final report to be 25 pages long, we had to delete a lot of output/code. We have more detailed explanations/comments and extensive outputs in the .Rmd file
Group 1: Rio Pramana, Park Jong-min, Kang Seo-young
Each code block will have the name of the person who worked on it
===== LOADING LIBRARIES =====
```{r}
#install.packages("conflicted")
library(conflicted)
#install.packages("visdat")
library(visdat)
library(psych)
#install.packages("lsr")
library(lsr)
#install.packages("car")
library(car)
library(MASS)
#install.packages("gridExtra")
library(gridExtra)
# install.packages("knitr")
library(knitr)
#install.packages("randomForest")
#install.packages("caret")
#install.packages("doParallel")
library(randomForest)
library(caret)
library(doParallel)
library(tidyverse)
conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
```
===== IMPORTING DATASET & INITIAL CHECKING =====
The dataset has been downloaded in a .csv format, and we assume the dataset is in the same folder as the .Rmd
```{r}
insurance_data <- read.csv("insurance_dataset.csv")
```
We then take a look at the structure of the dataset, as well as seeing how many rows and columns we have
```{r}
str(insurance_data)
nrow(insurance_data)
ncol(insurance_data)
# 1 million rows with 12 variables
```
===== SUBSETTING DATASET =====
We subset the original 1 million rows dataset into a 62250 rows dataset (Young adults with no children insurance dataset). We filter the original 'insurance_data' for age between 18 and 35 (inclusive), as well as 'children' with the value of 0. Because all 'children' column value will be 0 in this subset, we decided to not include 'children' column in this subset because it will not matter
```{r echo=TRUE}
young_adults_insurance_data <- insurance_data %>% filter(age >= 18 & age <= 35 & children == 0) %>% select(-children)
```

```{r}
# These code are for checking the final look of the subset
nrow(young_adults_insurance_data)
head(young_adults_insurance_data,2)
# This code below is to visualize the structure of the subset
vis_dat(young_adults_insurance_data)
```
===== CHECKING DATASET FOR MISSING VALUES =====
```{r}
# This code is to check the whole dataset. If the sum of NA's is 0, that means there is no missing data
sum(is.na(young_adults_insurance_data))
# After running the code, we found that there is no missing value in the dataset
```
===== CHECKING DATASET FOR OUTLIERS =====
As most of our columns are categorical, we can only check and remove outliers from the numerical variables, which are 'bmi' and 'charges'. 'age' is also numerical but we already used it to subset the dataset as we are only taking age 18-35, so outliers detection is not needed for 'age'.
=== Kang Seo-young: 'charges' outliers removal ===
```{r}
#take detailed-method because charges are numeric and continuous variable.
charges <- young_adults_insurance_data$charges
```
The steps are to look at the boxplot first, visually examine outliers. Then, use the lower/upper boundary method to calculate and remove the outliers.
```{r}
boxplot(charges)
#There are a lot of outliers above extreme value.
#we need the value which determines whether outliers is or not.
IQR_charges <- IQR(charges)
upper_boundary_charges <- quantile(charges, 0.75) + 1.5 * IQR_charges
#Q3+1.5*IQR = 18958+1.5*6113=28127.5
#the value more than 28127.5 are outliers.

# filter out data with charges that violates the upper boundary
# update the young adults insurance dataset
young_adults_insurance_data <- filter(young_adults_insurance_data, charges < upper_boundary_charges)
# visually examine the new boxplot
boxplot(young_adults_insurance_data$charges)
#Comparing two boxplots, we can see outliers are removed.
```
=== Rio Pramana: 'bmi' outliers removal ===
Next variable to check for outliers is 'bmi. I show the boxplot first to check outliers visually. Then, to detect and remove outliers, I calculate the lower bound and upper bound of 'bmi'. Any 'bmi' value that is outside of these boundaries will be considered as outliers and will be removed. Detailed technical explanations are in the comments
```{r}
# Show boxplot of bmi
ggplot(young_adults_insurance_data, aes(x = "", y = bmi)) +
  geom_boxplot()
# Calculate Q1, Q3, and IQR for bmi
bmi_Q1 <- quantile(young_adults_insurance_data$bmi, 0.25)
bmi_Q3 <- quantile(young_adults_insurance_data$bmi, 0.75)
bmi_IQR <- bmi_Q3 - bmi_Q1
# Calculate the lower and upper bound for outliers
bmi_lower_bound <- bmi_Q1 - 1.5 * bmi_IQR
bmi_upper_bound <- bmi_Q3 + 1.5 * bmi_IQR
# Filter for outliers
# The filter will filter out any bmi that is lower than the lower bound OR any bmi that is higher than the upper bound
bmi_outliers <- young_adults_insurance_data %>% filter(bmi < bmi_lower_bound | bmi > bmi_upper_bound)
nrow(bmi_outliers) #output shows 0 outliers for bmi
```
After the outliers removal process, we are left with 62119 rows of data to work with.
```{r}
nrow(young_adults_insurance_data)
```
===== INITIAL DATA VISUALIZATION =====
In this part of the code, we are only looking to visualize each variable to have a better understanding of each variable's data
=== Kang Seo-young: charges, occupation, coverage_level ===
```{r}
#histogram
ggplot(young_adults_insurance_data, aes(x=occupation)) +
  geom_bar(binwidth = 1, fill='skyblue', color='black') +
  ggtitle('Histogram of occupation')

ggplot(young_adults_insurance_data, aes(x=coverage_level)) +
  geom_bar(binwidth=1, fill='skyblue', color='black') +
  ggtitle('Histogram of coverage_level')
#very balanced and we should include all characters in occupation, coverage_level
#직업과 보험 레벨 간 차이가 별로 없음 very balanced.
#prove that we should take other columns to find insights.
```
=== Park Jong-min: age, gender, smoker, region ===
```{r}
#subset data for smoker values
#subset for only data with smoker == 'yes'
smoker_data<-subset(young_adults_insurance_data, smoker=='yes')
#subset for only data with smoker == 'no'
non_smoker_data<-subset(young_adults_insurance_data, smoker=='no')
#visualize smoker combined with gender and region
ggplot(smoker_data, aes(x=gender, fill=gender),stat = 'identity')+
  geom_bar()
ggplot(smoker_data, aes(x=region,fill=gender),stat = 'identity')+
  geom_bar()+
  ylab("smoker")
ggplot(non_smoker_data, aes(x=region,fill=gender),stat = 'identity')+
  geom_bar()+
  ylab("non_smoker")
#smoker('yes') and coverage_level
p_smoker_level<-ggplot(smoker_data,aes(x=coverage_level,y=smoker,fill=gender))+
  geom_bar(stat='identity')
#smoker('no') and coverage_level
p_non_smoker_level<-ggplot(non_smoker_data,aes(x=coverage_level,y=smoker,fill=gender))+
  geom_bar(stat='identity')
```
=== Rio Pramana: bmi, medical_history, family_medical_history, exercise_frequency ===
For medical_history, family_medical_history, exercise_frequency, which are categorical variables, i want to make a pie chart to show how perfectly balanced they are
```{r}
# Plot the histogram of bmi to see the distribution using ggplot
ggplot(young_adults_insurance_data, aes(x=bmi)) +
  geom_histogram(binwidth=1, fill="blue", color="black") +
  xlab("BMI") +
  ylab("Frequency") +
  ggtitle("Histogram of BMI")
# For medical_history, family_medical_history, exercise_frequency, which are categorical variables, i want to make a pie chart to show how perfectly balanced they are
# First step is to generate a table for each variable's frequency
medical_history_table <- table(young_adults_insurance_data$medical_history)
family_medical_history_table <- table(young_adults_insurance_data$family_medical_history)
exercise_frequency_table <- table(young_adults_insurance_data$exercise_frequency)
# Create pie chart for the medical_history variable
# First, create a data frame with columns 'label' and 'count' that keep tracks of unique values and their frequency from medical_history variable
df_medical_history <- data.frame(label = names(medical_history_table), count = as.vector(medical_history_table))
# Next, make another column 'percentage' that shows each unique values data percentage
df_medical_history$percentage <- df_medical_history$count / sum(df_medical_history$count)
# Final step is to visualize the pie chart using the data frame made
# geom_text here is used to give a label of the percentage in the pie chart itself like "25%"
# coord_polar() is the function responsible to change a bar chart into a pie chart by changing the coordinate system to polar coordinates. "y" argument inside it indicates to the function that the y axis will be wrapped around the circle.
# To make the visualization clear, I removed the legend by using theme() function
ggplot(df_medical_history, aes(x = "", y = count, fill = label)) +
  geom_bar(width = 1, stat = "identity") +
  geom_text(aes(label = paste0(round(percentage*100, 1), "%")), position = position_stack(vjust = 0.5)) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.title = element_blank()) +
  labs(title = "Pie Chart of Medical History")
# Now, for the rest of the variable, the steps and the code are exactly the same as above
# Create pie chart for the family_medical_history variable
df_family_medical_history <- data.frame(label = names(family_medical_history_table), count = as.vector(family_medical_history_table))
df_family_medical_history$percentage <- df_family_medical_history$count / sum(df_family_medical_history$count)

ggplot(df_family_medical_history, aes(x = "", y = count, fill = label)) +
  geom_bar(width = 1, stat = "identity") +
  geom_text(aes(label = paste0(round(percentage*100, 1), "%")), position = position_stack(vjust = 0.5)) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.title = element_blank()) +
  labs(title = "Pie Chart of Family Medical History")
# Create pie chart for the exercise_frequency variable
df_exercise_frequency <- data.frame(label = names(exercise_frequency_table), count = as.vector(exercise_frequency_table))
df_exercise_frequency$percentage <- df_exercise_frequency$count / sum(df_exercise_frequency$count)

ggplot(df_exercise_frequency, aes(x = "", y = count, fill = label)) +
  geom_bar(width = 1, stat = "identity") +
  geom_text(aes(label = paste0(round(percentage*100, 1), "%")), position = position_stack(vjust = 0.5)) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.title = element_blank()) +
  labs(title = "Pie Chart of Exercise Frequency")
```
We found from the plot that these variables are perfectly balanced
===== EDA 1: Univariate Relationship with Charges =====
The first thing we want to see is each variable's correlation/relationship with the dependent variable 'charges'. We want to also quantify this correlation/relationship instead of just looking at it visually
=== Kang Seo-young: region, occupation, exercise_frequency, coverage_level ===
```{r}
#plot each variable vs charges
# Exercise frequency vs charges
#turn exercise_frequency into ordered factor as the x axis against charges on the y axis
ggplot(young_adults_insurance_data, aes(x=factor(exercise_frequency, levels=c("Never", "Rarely", "Occasionally", "Frequently")), y=charges)) +
  geom_boxplot(fill = "cornflowerblue", outlier.shape = NA) +
  theme_minimal() +
  theme(text = element_text(size=14), axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Exercise Frequency vs Charges", x="Exercise Frequency", y="Charges")
#the rest of the variables follow the same code
# Region vs charges
ggplot(young_adults_insurance_data, aes(x=region, y=charges)) +
  geom_boxplot(fill = "coral", outlier.shape = NA) +
  theme_minimal() +
  theme(text = element_text(size=14), axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Region vs Charges", x="Region", y="Charges")
# Occupation vs charges
ggplot(young_adults_insurance_data, aes(x=occupation, y=charges)) +
  geom_boxplot(fill = "lightgreen", outlier.shape = NA) +
  theme_minimal() +
  theme(text = element_text(size=14), axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Occupation vs Charges", x="Occupation", y="Charges")
# Coverage level vs charges
ggplot(young_adults_insurance_data, aes(x=factor(coverage_level, levels=c("Basic", "Standard", "Premium")), y=charges)) +
  geom_boxplot(fill = "skyblue", outlier.shape = NA) +
  theme_minimal() +
  theme(text = element_text(size=14), axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title="Coverage Level vs Charges", x="Coverage Level", y="Charges")
```

```{r}
#quantify relationship
# Exercise frequency vs charges
# perform ANOVA using aov() function with charges as dependent variable
anova_exercise_frequency <- aov(charges ~ exercise_frequency, data = young_adults_insurance_data)
# get the summary of the ANOVA
summary_exercise_frequency <- summary(anova_exercise_frequency)
#calculate the eta squared of the ANOVA
# it is the sum of squares divided by the total sum of squares including the residuals
eta_sq_exercise_frequency <- summary_exercise_frequency[[1]]["exercise_frequency", "Sum Sq"] / (summary_exercise_frequency[[1]]["exercise_frequency", "Sum Sq"] + summary_exercise_frequency[[1]]["Residuals", "Sum Sq"])
# print the results
print(paste("Eta-squared for exercise_frequency: ", eta_sq_exercise_frequency))
# The code to calculate and print eta squared values of the ANOVA is the same for the rest of the variables
# Region vs charges
anova_region <- aov(charges ~ region, data = young_adults_insurance_data)
summary_region <- summary(anova_region)
eta_sq_region <- summary_region[[1]]["region", "Sum Sq"] / (summary_region[[1]]["region", "Sum Sq"] + summary_region[[1]]["Residuals", "Sum Sq"])
print(paste("Eta-squared for region: ", eta_sq_region))
# Occupation vs charges
anova_occupation <- aov(charges ~ occupation, data = young_adults_insurance_data)
summary_occupation <- summary(anova_occupation)
eta_sq_occupation <- summary_occupation[[1]]["occupation", "Sum Sq"] / (summary_occupation[[1]]["occupation", "Sum Sq"] + summary_occupation[[1]]["Residuals", "Sum Sq"])
print(paste("Eta-squared for occupation: ", eta_sq_occupation))
# Coverage level vs charges
anova_coverage_level <- aov(charges ~ coverage_level, data = young_adults_insurance_data)
summary_coverage_level <- summary(anova_coverage_level)
eta_sq_coverage_level <- summary_coverage_level[[1]]["coverage_level", "Sum Sq"] / (summary_coverage_level[[1]]["coverage_level", "Sum Sq"] + summary_coverage_level[[1]]["Residuals", "Sum Sq"])
print(paste("Eta-squared for coverage_level: ", eta_sq_coverage_level))
```
=== Rio Pramana: bmi, medical_history, family_medical_history ===
```{r}
# Relationship between BMI and Charges
# geom_jitter is used to make the plot look clearer as bmi has too much data to be fit into a single graph (avoid overplotting)
# geom_smooth is used to visualize the trend in the bmi plot
ggplot(young_adults_insurance_data, aes(x = bmi, y = charges)) +
  geom_jitter(alpha = 0.1, width = 0.2, height = 0.2) +  # add some jittering
  geom_smooth(method = "loess", color = "red", se = FALSE) +  # add trend line
  xlab("BMI") +
  ylab("Charges") +
  ggtitle("Scatter plot of Charges vs BMI with Trend Line") +
  theme_minimal() +
  theme(text = element_text(size=16))
# Relationship between Medical History and Charges
ggplot(young_adults_insurance_data, aes(x = medical_history, y = charges, fill = medical_history)) +
  geom_boxplot(alpha = 0.6, outlier.shape = NA) +  # Avoid displaying outliers to focus on the relationship
  scale_fill_brewer(palette = "Set3") + #this is just to use different colours for the fill in the plot
  xlab("Medical History") +
  ylab("Charges") +
  ggtitle("Box plot of Charges by Medical History") +
  theme_minimal() +
  theme(text = element_text(size=16)) +
  guides(fill=FALSE) # Removes legend
# Relationship between Family Medical History and Charges
ggplot(young_adults_insurance_data, aes(x = family_medical_history, y = charges, fill = family_medical_history)) +
  geom_boxplot(alpha = 0.6, outlier.shape = NA) +  # Avoid displaying outliers
  scale_fill_brewer(palette = "Set3") +
  xlab("Family Medical History") +
  ylab("Charges") +
  ggtitle("Box plot of Charges by Family Medical History") +
  theme_minimal() +
  theme(text = element_text(size=16)) +
  guides(fill=FALSE) # Removes legend
```
This part is to quantify the correlations
```{r}
# for bmi which is numerical, we can just run cor.test() to get the Pearson correlation coefficient
corr_bmi_charges <- cor.test(young_adults_insurance_data$bmi, young_adults_insurance_data$charges, method = "pearson")
# we run a linear regression model too to see more data about the correlation
lm_model_bmi <- lm(charges ~ bmi, data = young_adults_insurance_data)
# for categorical variables, we have to use aov() to perform ANOVA and calculate the eta squared value which tells us the correlation/relationship
anova_model_medical_history <- aov(charges ~ medical_history, data = young_adults_insurance_data)
anova_model_family_medical_history <- aov(charges ~ family_medical_history, data = young_adults_insurance_data)
```
This part is to display the correlations
```{r}
# display correlations of bmi against charges
corr_bmi_charges #0.1 pearson coefficient
summary(lm_model_bmi)$adj.r.squared #0.01 adjusted R-squared
# display the eta squared value of medical_history and family_medical_history against charges using etaSquared() function from lsr library
etaSquared(anova_model_medical_history) #0.1766
etaSquared(anova_model_family_medical_history) #0.1754
```
=== Park Jong-min: gender, smoker, age ===
```{r}
###smoker, gender, age - charges######
# visualize smoker, gender, age, against charges
#gender, charges
# this plot is a bar plot of gender against charges
p_gender<-ggplot(young_adults_insurance_data,aes(x=gender,y=charges,fill=gender))+
  geom_bar(stat='identity')+
  ggtitle('Gender-Charges')+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5,size=20,face='bold'))+
  scale_fill_manual(values = c("pink", "blue"))  
#smoker, charges
# this graph is a boxplot of smoker against charges
p_gender_box<-ggplot(young_adults_insurance_data,aes(x=gender,y=charges,fill=smoker))+
  geom_boxplot()+
  ggtitle('Smoker-Charges')+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5,size=20,face='bold'))+
  scale_fill_manual(values = c("ivory", 'brown'))  
#age, charges
# this plot is a stacked bar plot of age against charges with smoker as additional variable
p_age<-ggplot(young_adults_insurance_data,aes(x=age,y=charges,fill=smoker))+
  geom_histogram(stat='identity')+
  ggtitle('Age-Charges')+
  theme_classic()+
  theme(plot.title = element_text(hjust = 0.5,size=20,face='bold'))+
  scale_fill_manual(values = c("yellow", "black")) 
# display:
p_gender
p_gender_box
p_age
```
Quantify correlations using linear regression models:
For categorical variables, R automatically calculates eta squared value for the adjusted R-squared data if we use linear regression model to get a univariate relationship
```{r}
#linear regression(charges-smoker)
fit_1 <- lm(charges ~ smoker, data = young_adults_insurance_data)
linear_formula_1 <- as.formula(fit_1)
summary(fit_1)$adj.r.squared
```

```{r}
#linear regression(charges-gender)
fit_2 <- lm(charges ~ gender, data = young_adults_insurance_data)
linear_formula_2 <- as.formula(fit_2)
summary(fit_2)$adj.r.squared
```

```{r}
#linear regression(charges-age)
fit_3 <- lm(charges ~ age, data = young_adults_insurance_data)
linear_formula_3 <- as.formula(fit_3)
summary(fit_3)$adj.r.squared
```
===== EDA 2: Feature Importance Validation using Machine Learning =====
We build a Random Forest model to validate the feature importance (adjusted R-squared and eta squared) that we got from EDA 1
=== Park Jong-min ===
```{r}
###RandomForest model#####
reduced_insurance_data<-young_adults_insurance_data
#convert into nummeric values
reduced_insurance_data$gender <- as.integer(factor(reduced_insurance_data$gender))
reduced_insurance_data$smoker <- as.integer(factor(reduced_insurance_data$smoker))
reduced_insurance_data$region <- as.integer(factor(reduced_insurance_data$region))
reduced_insurance_data$medical_history <- as.integer(factor(reduced_insurance_data$medical_history))
reduced_insurance_data$family_medical_history <- as.integer(factor(reduced_insurance_data$family_medical_history))
reduced_insurance_data$exercise_frequency <- as.integer(factor(reduced_insurance_data$exercise_frequency))
reduced_insurance_data$occupation <- as.integer(factor(reduced_insurance_data$occupation))
reduced_insurance_data$coverage_level <- as.integer(factor(reduced_insurance_data$coverage_level))
#split the data into train data and test data
sn <- sample(1:nrow(reduced_insurance_data), size = nrow(reduced_insurance_data)*0.7, replace = FALSE)
train<-reduced_insurance_data[sn,]
test<-reduced_insurance_data[-sn,]
#parallel processing
cl <- makeCluster(detectCores())
registerDoParallel(cl)
#model training
forest_m <- randomForest(charges ~ ., data=train, importance = T)
#predict charges
y_pred <- predict(forest_m, test, type='response')
# R-squared
predicted <- y_pred
actual <- test$charges
mean_actual <- mean(actual)
total_sum_squares <- sum((actual - mean_actual)^2)
residual_sum_squares <- sum((actual - predicted)^2)
r_squared <- 1 - (residual_sum_squares / total_sum_squares)
#r_squared
# Adjusted-R-squared
n <- nrow(test)
p <- ncol(reduced_insurance_data) - 1
adjusted_R_squared <- 1 - (1 - r_squared) * (n - 1) / (n - p - 1)
#adjusted_R_squared

stopCluster(cl)
#feature importance
importance(forest_m)
total_importance <- sum(forest_m$importance[, "%IncMSE"]) 
percentage_importance <- forest_m$importance[, "%IncMSE"] / total_importance * 100
percentage_importance
```
Based on the results, we got almost the exact same numbers for the feature importance. That means our analysis from EDA 1 was correct
===== EDA 3: Predictive Model to Predict Insurance Charges =====
We want to answer our question if the most important variables are enough to predict insurance charges or not
=== Rio Pramana ===
We split the dataset into a 70-30 split first
```{r}
set.seed(1)  # to make sure we get the same split of data everytime we run the code
# Split the dataset into training and test dataset:
train_indices <- sample(1:nrow(young_adults_insurance_data), nrow(young_adults_insurance_data)*0.7)
train_data <- young_adults_insurance_data[train_indices,]
test_data <- young_adults_insurance_data[-train_indices,]
# check the split
nrow(train_data)
nrow(test_data)
# For categorical variables, we have to convert them into factors first
# Convert variables to factors
train_data$smoker <- as.factor(train_data$smoker)
train_data$coverage_level <- as.factor(train_data$coverage_level)
train_data$medical_history <- as.factor(train_data$medical_history)
train_data$family_medical_history <- as.factor(train_data$family_medical_history)
# Check the levels of the factors
levels(train_data$smoker)
levels(train_data$coverage_level)
levels(train_data$medical_history)
levels(train_data$family_medical_history)
# Our initial_model as presented in the PPT, it is a predictive linear regression model using the top 4 most important variables (decided based on their feature importance which is their adjusted R-squared or eta squared value)
```
Initial model & its R-squared:
```{r}
# 1. Initial Model
initial_model <- lm(charges ~ smoker + coverage_level + medical_history + family_medical_history, data = train_data)
summary(initial_model)$adj.r.squared
```
This is for the second_model:
```{r}
# in the second model, we add 2 more variables, occupation and exercise_frequency as they are the next 2 most important variables
# We used this model to compare with the initial_model, as the initial_model already accounts for over 90% of the variability in charges. Meanwhile occupation and exercise_frequency only combines for around 5% of eta squared value
# Convert variables to factors
train_data$occupation <- as.factor(train_data$occupation)
train_data$exercise_frequency <- as.factor(train_data$exercise_frequency)
# Check the levels of the factors
levels(train_data$occupation)
levels(train_data$exercise_frequency)
# 2. Add occupation and exercise_frequency
second_model <- lm(charges ~ smoker + coverage_level + medical_history + family_medical_history + occupation + exercise_frequency, data = train_data)
summary(second_model)$adj.r.squared
```
The next step is to check model assumptions. This step is basically to make sure that both our predictive linear regression models are reliable. 1. Residuals should be normally distributed, 2. Residuals should show no clear pattern above and under the trend line against fitted values
```{r}
# Check Model Assumptions for 
# Prepare data for plots
# initial_model
resid_initial <- data.frame(
  Model = "Initial Model", Residuals = resid(initial_model), # get the residuals
  Fitted = fitted(initial_model) # get the predictions
)
# second_model
resid_second <- data.frame(
  Model = "Second Model", Residuals = resid(second_model), Fitted = fitted(second_model)
)
# bind the residuals from both models to be shown in the same graph side-by-side
resid_data <- rbind(resid_initial, resid_second)
# Residual distribution comparison
ggplot(resid_data, aes(Residuals, fill = Model)) +
  geom_histogram(alpha = 0.5, position = 'identity', bins = 30) +
  facet_wrap(~ Model, scales = "free") + #use facet wrap to show both model's histogram side by side
  theme_minimal() +
  labs(title="Residual Distribution Comparison", x="Residuals", y="Frequency")
# Residuals vs Fitted values comparison (prediction errors plot)
ggplot(resid_data, aes(x = Fitted, y = Residuals, color = Model)) +
  geom_point(alpha = 0.5) +
  facet_wrap(~ Model, scales = "free") +
  geom_smooth(method = "loess", se = FALSE, color = "black") +
  theme_minimal() +
  labs(title="Residuals vs Fitted Values", x="Fitted Values", y="Residuals") +
  ylim(-4000, 4000) #add ylim to make it have the same y-axis ranges as the initial_model plot to show the improvement visually
```
From the graphs, both models fulfilled the first 2 assumptions. Although, the second model shows a much better graph for the second assumption.
The third assumption is no multicollinearity (correlation between independent variables)
```{r}
# Multivariate Analysis - use the variance inflation factor (VIF) to check for multicollinearity
vif_initial <- vif(initial_model) # use vif() to calculate this
vif_second <- vif(second_model) # for second_model
#display:
vif_initial
vif_second
# looking at the GVIF values, almost all of them are very close to 1 which means that there is no multicollinearity
```
The next step is to compare both models performance and result. The first metric that we compare is the AIC (Akaike information criterion), second_model got a better result
```{r}
# to calculate AIC, we have several options. I chose the stepwise selection with direction = "both". This basically means that for each model, we will remove variables step by step, then add variables step by step, and see which variation/combination of the variables work the best for that model. We will get a number at the end that will summarize the whole process
# initial_model
step_initial_model <- stepAIC(initial_model, direction = "both")
# second_model
step_second_model <- stepAIC(second_model, direction = "both")
# Extract AIC values
# the lower the AIC value from the step above, the better the model is (less overfitting)
aic_initial <- AIC(step_initial_model)
aic_second <- AIC(step_second_model)
# Combine the data into a data frame to plot
data <- data.frame(
  Model = c("Initial Model", "Second Model"),
  AIC = c(aic_initial, aic_second)
)
# Create a bar plot to compare both model's results side by side
ggplot(data, aes(x = Model, y = AIC, fill = Model)) +
  geom_bar(stat = "identity") +
  theme_minimal() +
  labs(
    x = "Model", 
    y = "AIC Value", 
    title = "AIC Comparison of Models",
    fill = "Model"
  )
```
The next 2 metrics that we checked are Mean Squared Error (MSE) and Root Mean Squared Error (RMSE). The lower the numbers of both metrics, the better the performance are
```{r}
# make predictions on the test set and check model performance
actual_values <- test_data$charges # get the actual values
# predict using initial_model
init_model_predictions <- predict(initial_model, newdata = test_data)
# calculate the MSE and RMSE using the formula
init_model_MSE <- mean((init_model_predictions - actual_values)^2)  # Mean Squared Error
init_model_RMSE <- sqrt(init_model_MSE)  # Root Mean Squared Error
# predict using second_model
second_model_predictions <- predict(second_model, newdata = test_data)
# calculate the MSE and RMSE using the formula
second_model_MSE <- mean((second_model_predictions - actual_values)^2)  # Mean Squared Error
second_model_RMSE <- sqrt(second_model_MSE)  # Root Mean Squared Error
# Display:
# Create a data frame to store the results for plotting
results <- data.frame(
  Model = c("Initial Model", "Second Model"),
  MSE = c(init_model_MSE, second_model_MSE),
  RMSE = c(init_model_RMSE, second_model_RMSE)
)
# Print the results
print(results)
# Highlight the best model for each metric
best_model_MSE <- results$Model[which.min(results$MSE)]
best_model_RMSE <- results$Model[which.min(results$RMSE)]
# Print the best model on each metric
cat("Model with lowest MSE: ", best_model_MSE, "\n")
cat("Model with lowest RMSE: ", best_model_RMSE, "\n")
# Create a bar plot to compare both model's MSE
mse_plot <- ggplot(results, aes(x = Model, y = MSE, fill = Model)) +
  geom_bar(stat = "identity") +
  # the line below add text layer above each bar rounded to 1 decimal place to act as a label and show the actual number of MSE
  geom_text(aes(label=format(round(MSE, 1), big.mark = ",")), vjust=-0.5) +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  ylim(0, 2000000) +
  labs(
    x = "Model", 
    y = "MSE", 
    title = "Model Validation - MSE",
    fill = "Model"
  )
# Create a bar plot to compare both model's RMSE
rmse_plot <- ggplot(results, aes(x = Model, y = RMSE, fill = Model)) +
  geom_bar(stat = "identity") +
  # the line below add text layer above each bar rounded to 1 decimal place to act as a label and show the actual number of RMSE
  geom_text(aes(label=format(round(RMSE, 1), big.mark = ",")), vjust=-0.5) +
  scale_y_continuous(labels = scales::comma) +
  theme_minimal() +
  ylim(0, 1500) +
  labs(
    x = "Model", 
    y = "RMSE", 
    title = "Model Validation - RMSE",
    fill = "Model"
  )
# Arrange the plots in one row
grid.arrange(mse_plot, rmse_plot, nrow = 2)
```
Based on all of the results, everything shows that the second_model (which uses 6 most important variables) are much better in terms of performance. So, it shows that even if the 4 most important variables account for over 90% of charges' variability, we still need to include other health factors to accurately predict insurance charges even if their univariate relationship is small